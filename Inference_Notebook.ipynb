{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference Notebook",
      "provenance": [],
      "collapsed_sections": [
        "tAmwnPYy8wdR",
        "4lcwmjM570tt",
        "LHcel-Ut8KCb",
        "qXzgcigi8o27",
        "-jTVp-gC8a-O"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dan-La/scientific-challenges-and-directions/blob/main/Inference_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky5s7Oh-TQbR"
      },
      "source": [
        "# A notebook to perform inference on sentences with the problem/direction labels\n",
        "\n",
        "Note: you need to adjust the paths to the location in which you store the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAmwnPYy8wdR"
      },
      "source": [
        "## Installs and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0PkpQrkcWPF"
      },
      "source": [
        "### install/import\n",
        "import pip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "!pip install -v transformers==4.9.2 \n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5xtLFSGgZ8s"
      },
      "source": [
        "### check I am using a GPU\n",
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "device_lib.list_local_devices()\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lcwmjM570tt"
      },
      "source": [
        "## Model path, param and classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-kPmHVgp-FT"
      },
      "source": [
        "### Defining the model and inference params\n",
        "\n",
        "MAX_LEN = 128 # set per the expected len \n",
        "NUM_LABELS=2 # col names of the labels in the dataset - ['problem', 'direction'] \n",
        "\n",
        "INFERENCE_BATCH_SIZE = 16 # set per the required \n",
        "inference_params = {'batch_size': INFERENCE_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0,\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QezbOP7gt_g6"
      },
      "source": [
        "### transform the data the required tokenized form and prepare for the DataLoader\n",
        "class ProblemDirectionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        text = \" \".join(text.split())\n",
        "        \n",
        "        inputs = self.tokenizer.encode_plus( #TODO: change to encoding per batch; to avoid a global max_len padding\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "            'text': self.texts[idx]\n",
        "        } \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX9rKH71ox3m"
      },
      "source": [
        "### define the NN\n",
        "class PubmedBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PubmedBERTClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", num_labels=NUM_LABELS)\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids = token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.l2(pooler)\n",
        "        output = self.l3(pooler)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHcel-Ut8KCb"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne6uraVuP574"
      },
      "source": [
        "### import the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", do_lower_case=True)\n",
        "\n",
        "### load the model\n",
        "!wget https://challenges-directions.s3.us-west-2.amazonaws.com/Multilabel_ProblemDirection.pth\n",
        "model = PubmedBERTClass()\n",
        "OPTIMIZER = torch.optim.Adam(params =  model.parameters(), lr=1e-05)\n",
        "LOSS_FUNCTION = torch.nn.BCELoss() # for the MultiLabel Case\n",
        "MODEL = torch.load('Multilabel_ProblemDirection.pth')\n",
        "\n",
        "MODEL.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXzgcigi8o27"
      },
      "source": [
        "## Inference Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMJMpYYD8Uzx"
      },
      "source": [
        "### inference functions\n",
        "def valid(model, testing_loader, loss_function, optimizer, threshold=0.5):\n",
        "    model.eval()\n",
        "    final_outputs = []; final_targets = []; final_logits = []; final_texts = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
        "            if list(outputs.size())==[2]: # in case only 1 sentence in batch\n",
        "                outputs = torch.reshape(outputs, (1,NUM_LABELS))\n",
        "            logits = outputs[0]\n",
        "            loss = loss_function(torch.sigmoid(outputs.view(-1,NUM_LABELS)), targets.type_as(logits).view(-1, NUM_LABELS)) # convert labels to float for calculation\n",
        "\n",
        "            final_targets.extend(targets.cpu().detach().numpy())\n",
        "            outputs_idx = torch.sigmoid(outputs).cpu().detach().numpy()\n",
        "            final_outputs.extend([idx for idx in outputs_idx])\n",
        "            final_logits.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "            final_texts.extend(data['text'])\n",
        "\n",
        "    #the default threshold is 0.5 for both labels\n",
        "    final_outputs = [np.where(array>threshold, 1, 0) for array in final_outputs] \n",
        "    return final_targets, final_outputs, final_logits, final_texts\n",
        "\n",
        "\n",
        "def inference(sentences, model=MODEL, loss_function=LOSS_FUNCTION, optimizer=OPTIMIZER, threshold=0.5):\n",
        "    \"\"\"\n",
        "    function to wrap the validation function for inference purposes\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    if len(sentences): # sentences != NULL\n",
        "        sentences_labels = [[0,0]] * len(sentences) # dummy label since running in inference \n",
        "        sentences_dataset = ProblemDirectionDataset(sentences, sentences_labels, tokenizer, MAX_LEN) # prep the dataset\n",
        "        sentences_loader = DataLoader(sentences_dataset, **inference_params) \n",
        "        final_targets, final_outputs, final_logits, final_texts = valid(model=model, testing_loader=sentences_loader, loss_function=loss_function, optimizer=optimizer, threshold=threshold)\n",
        "        for logit, text in zip(final_logits, final_texts):\n",
        "            results.append({'sequence':text, 'output': {'problem': logit[0], 'direction': logit[1]}})\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jTVp-gC8a-O"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm8tTOg9yp1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f5d356-f32c-4d08-ea4a-c49b005aad42"
      },
      "source": [
        "### infer sentences\n",
        "\n",
        "sentences = [\"we speculate that studying IL-6 will be beneficial\",\n",
        "             \"there is no solution to IRB limitation\",\n",
        "             \"germs find replications difficult\",\n",
        "             \"IbMADS1-transformed potatoes exhibited tuber morphogenesis in the fibrous roots.\",\n",
        "             \"Severe atypical cases of pneumonia emerged and quickly spread worldwide.\"]\n",
        "\n",
        "results = inference(sentences, model=MODEL)\n",
        "\n",
        "print(*results, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sequence': 'we speculate that studying IL-6 will be beneficial', 'output': {'problem': 0.18894509971141815, 'direction': 0.9751406908035278}}\n",
            "{'sequence': 'there is no solution to IRB limitation', 'output': {'problem': 0.7472777366638184, 'direction': 0.006989808287471533}}\n",
            "{'sequence': 'germs find replications difficult', 'output': {'problem': 0.47354429960250854, 'direction': 0.0066837649792432785}}\n",
            "{'sequence': 'IbMADS1-transformed potatoes exhibited tuber morphogenesis in the fibrous roots.', 'output': {'problem': 0.015217977575957775, 'direction': 0.011578607372939587}}\n",
            "{'sequence': 'Severe atypical cases of pneumonia emerged and quickly spread worldwide.', 'output': {'problem': 0.9731208086013794, 'direction': 0.018003808334469795}}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}